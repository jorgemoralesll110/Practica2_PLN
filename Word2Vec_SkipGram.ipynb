{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T19:10:05.650008Z",
     "start_time": "2026-01-14T19:10:05.641638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys, os\n",
    "from word2vec.main import run_program"
   ],
   "id": "e734df18a0c4cdeb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Word2Vec con Skip-Gram: implementación y análisis\n",
    "\n",
    "En este cuaderno se presenta la implementación completa de un modelo **Word2Vec basado en Skip-Gram**, desarrollada desde cero con fines académicos, sin utilizar librerías de alto nivel como *gensim*. El objetivo principal de este trabajo es comprender el funcionamiento interno del modelo, desde el procesamiento del corpus hasta el análisis de los embeddings aprendidos.\n",
    "\n",
    "El flujo de trabajo se ha estructurado de forma modular, separando claramente las distintas etapas del proceso: carga y normalización del corpus, construcción del vocabulario, generación de pares centro-contexto, entrenamiento del modelo y evaluación de los resultados. El cuaderno actúa como punto de orquestación, mientras que la lógica principal se encuentra implementada en un paquete independiente.\n",
    "\n",
    "Para el entrenamiento se utiliza el modelo Skip-Gram con **softmax completo sobre todo el vocabulario**. Aunque este enfoque presenta un coste computacional elevado, resulta adecuado en un contexto docente, ya que permite implementar de forma directa la función objetivo original y analizar con claridad el cálculo de probabilidades y gradientes. Alternativas más eficientes como *Negative Sampling* o *Hierarchical Softmax* no se han incluido con el fin de priorizar la claridad conceptual y la trazabilidad del entrenamiento.\n",
    "\n",
    "El corpus empleado ha sido construido de forma controlada para incluir patrones lingüísticos claros, tales como relaciones de género gramatical, acciones asociadas a agentes, atributos de objetos y relaciones geográficas simples. Aunque su tamaño es moderado, contiene una alta repetición de estructuras, lo que lo hace adecuado para el entrenamiento y análisis de modelos distribucionales en un entorno académico.\n",
    "\n",
    "La evaluación del modelo combina un análisis **cuantitativo** y **cualitativo**. Por un lado, se estudia la pérdida final del modelo y se calcula una métrica de coherencia local basada en la similitud media entre palabras y sus vecinos más cercanos. Por otro lado, se analizan los embeddings mediante la inspección de vecinos semánticos y la resolución de analogías diseñadas de acuerdo con el contenido del corpus, evitando casos triviales.\n",
    "\n",
    "Este enfoque permite no solo comprobar que el modelo converge correctamente, sino también reflexionar sobre las capacidades y limitaciones de Word2Vec cuando se entrena con corpus de tamaño reducido."
   ],
   "id": "fe2c1af5ad8a8797"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T19:09:10.314415Z",
     "start_time": "2026-01-14T19:09:10.301128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_dir = \"C:/Users/ralme/OneDrive/Escritorio/4_GCID/Asignaturas/PLN/Practica2_PLN\"\n",
    "sys.path.insert(0, os.path.join(base_dir))\n",
    "sys.path.insert(0, os.path.join(base_dir, 'w2v'))\n",
    "print('Base directory:', base_dir)"
   ],
   "id": "bf239a05a866da13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: C:/Users/ralme/OneDrive/Escritorio/4_GCID/Asignaturas/PLN/Practica2_PLN\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Estructura del cuaderno\n",
    "\n",
    "1. Configuración del entorno y carga del corpus  \n",
    "2. Ejecución del pipeline completo de Word2Vec  \n",
    "3. Evaluación cuantitativa de los embeddings  \n",
    "4. Análisis cualitativo: vecinos y analogías  "
   ],
   "id": "b89a47ec62027009"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T19:09:22.671810Z",
     "start_time": "2026-01-14T19:09:18.579651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus_path = os.path.join(base_dir, 'resources', 'dataset_word2vec.txt')\n",
    "if not os.path.exists(corpus_path):\n",
    "    sample = [\n",
    "        \"París es la capital de Francia\",\n",
    "        \"Madrid es la capital de España\",\n",
    "        \"el perro ladra en la casa\",\n",
    "        \"el gato maúlla en la silla\",\n",
    "        \"el coche del conductor está en la calle\"\n",
    "    ]\n",
    "    os.makedirs(os.path.dirname(corpus_path), exist_ok=True)\n",
    "    with open(corpus_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(sample))\n",
    "    print(\"Sample corpus created at:\", corpus_path)\n",
    "\n",
    "model, vocab, inv_vocab, pairs = run_program(\n",
    "    corpus_path=corpus_path,\n",
    "    window_size=2,\n",
    "    embedding_dim=50,\n",
    "    learning_rate=0.05,\n",
    "    epochs=100,\n",
    "    min_count=1\n",
    ")"
   ],
   "id": "ef8dbe17a62dfce0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "WORD2VEC - EJECUCIÓN DEL PROGRAMA MEDIANTE SKIP-GRAM\n",
      "**********************************************************************\n",
      "\n",
      " Primer paso) Cargar y tokenizar el corpus\n",
      "\n",
      " Segundo paso) Construir el vocabulario\n",
      "Tamaño del vocabulario: 355\n",
      "Top 10 palabras: [('la', 131), ('el', 129), ('es', 37), ('está', 27), ('en', 26), ('un', 20), ('por', 15), ('una', 15), ('al', 14), ('gato', 11)]\n",
      "\n",
      " Tercer paso) Generar el par 'Centro-Contexto'\n",
      "Total pairs: 2413\n",
      "\n",
      " Cuarto paso) Entrenar el modelo Skip-Gram\n",
      "\n",
      " Quinto paso) Evaluación cuantitativa de los embeddings\n",
      "Average neighbor similarity: 0.9522\n",
      "\n",
      "**********************************************************************\n",
      "ANÁLISIS DE LOS EMBEDDINGS\n",
      "**********************************************************************\n",
      "\n",
      " ********** VECINOS MÁS CERCANOS **********\n",
      "\n",
      "'perro':\n",
      "  - gato: 0.9829\n",
      "  - en: 0.9591\n",
      "  - tren: 0.9478\n",
      "  - coche: 0.9471\n",
      "  - está: 0.9449\n",
      "\n",
      "'gato':\n",
      "  - perro: 0.9829\n",
      "  - en: 0.9564\n",
      "  - coche: 0.9513\n",
      "  - tren: 0.9509\n",
      "  - parque: 0.9477\n",
      "\n",
      "'coche':\n",
      "  - gato: 0.9513\n",
      "  - perro: 0.9471\n",
      "  - vuela: 0.9431\n",
      "  - en: 0.9405\n",
      "  - es: 0.9385\n",
      "\n",
      "'parís':\n",
      "  - dos: 0.4178\n",
      "  - llave: 0.3736\n",
      "  - grande: 0.3724\n",
      "  - camino: 0.3649\n",
      "  - lejos: 0.3526\n",
      "\n",
      "'francia':\n",
      "  - el: 0.5440\n",
      "  - vaso: 0.3929\n",
      "  - tranquilo: 0.3793\n",
      "  - duerme: 0.3727\n",
      "  - caliente: 0.3355\n",
      "\n",
      "'niña':\n",
      "  - por: 0.9609\n",
      "  - perra: 0.9521\n",
      "  - está: 0.9456\n",
      "  - madre: 0.9409\n",
      "  - gata: 0.9389\n",
      "\n",
      "'agua':\n",
      "  - cierra: 0.7164\n",
      "  - pájaro: 0.6984\n",
      "  - ayuda: 0.6961\n",
      "  - ordenador: 0.6862\n",
      "  - coche: 0.6857\n",
      "\n",
      "'casa':\n",
      "  - ciudad: 0.9562\n",
      "  - pelota: 0.9558\n",
      "  - mañana: 0.9497\n",
      "  - por: 0.9451\n",
      "  - es: 0.9448\n",
      "\n",
      "'profesor':\n",
      "  - café: 0.4338\n",
      "  - persigue: 0.4171\n",
      "  - periódico: 0.3966\n",
      "  - médico: 0.3962\n",
      "  - escucha: 0.3906\n",
      "\n",
      "'médico':\n",
      "  - avión: 0.5672\n",
      "  - teléfono: 0.5208\n",
      "  - persigue: 0.5106\n",
      "  - gusta: 0.5025\n",
      "  - pasea: 0.4898\n",
      "\n",
      "**********************************************************************\n",
      "ANÁLISIS DE LAS ANALOGÍAS\n",
      "\n",
      "parís : francia :: madrid : ?\n",
      "     - seca : 0.3069\n",
      "     - ingeniera : 0.2905\n",
      "     - tranquilo : 0.2775\n",
      "\n",
      "perro : ladra :: gato : ?\n",
      "     - pan : 0.5886\n",
      "     - cierra : 0.5858\n",
      "     - gusta : 0.5618\n",
      "\n",
      "niño : niña :: profesor : ?\n",
      "     - gata : 0.7873\n",
      "     - madre : 0.7868\n",
      "     - perra : 0.7837\n",
      "\n",
      "**********************************************************************\n",
      "HIPERPARÁMETROS USADOS:\n",
      "    - Tamaño de la ventana: 2\n",
      "    - Dimensión del embedding: 50\n",
      "    - Tasa de aprendizaje: 0.05\n",
      "    - Épocas: 100\n",
      "    - Frecuencia mínima: 1\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conclusiones\n",
    "\n",
    "En este trabajo se ha implementado desde cero un modelo Word2Vec basado en Skip-Gram, abordando de forma completa todas las etapas del proceso, desde el tratamiento del corpus hasta el análisis de los embeddings obtenidos. La implementación modular ha permitido comprender con detalle el papel de cada componente y facilita la experimentación con distintas configuraciones del modelo.\n",
    "\n",
    "Los resultados obtenidos muestran que, incluso con un corpus de tamaño moderado, el modelo es capaz de capturar regularidades distribucionales básicas del lenguaje. En particular, los vecinos más cercanos presentan una coherencia semántica razonable en palabras frecuentes, y las analogías diseñadas de acuerdo con el contenido del corpus permiten observar relaciones como género gramatical, asociaciones entre agentes y acciones, y relaciones geográficas simples.\n",
    "\n",
    "El análisis cuantitativo, basado en la pérdida final del entrenamiento y en la similitud media entre palabras y sus vecinos más cercanos, confirma que el modelo converge correctamente y que determinadas elecciones de hiperparámetros influyen de forma apreciable en la calidad de las representaciones aprendidas. En general, se observa que un aumento en la dimensión del embedding y en el tamaño de la ventana tiende a producir representaciones más coherentes, aunque a costa de un mayor coste computacional.\n",
    "\n",
    "No obstante, el trabajo también pone de manifiesto las limitaciones del enfoque. El uso de softmax completo implica un coste computacional elevado que restringe la escalabilidad del modelo a vocabularios grandes, y el tamaño limitado del corpus condiciona la fiabilidad de analogías más complejas. Estas limitaciones son inherentes al enfoque adoptado y resultan especialmente relevantes en escenarios reales con grandes volúmenes de datos.\n",
    "\n",
    "Como líneas de trabajo futuro, se plantea la incorporación de técnicas de optimización como Negative Sampling o Hierarchical Softmax, que permitirían escalar el entrenamiento de forma más eficiente. Asimismo, el uso de corpus más amplios y variados podría mejorar la calidad semántica de los embeddings y permitir una evaluación más exhaustiva mediante tareas posteriores.\n",
    "\n",
    "En conjunto, este trabajo cumple el objetivo de comprender y analizar el funcionamiento interno de Word2Vec, proporcionando una base sólida tanto desde el punto de vista conceptual como experimental.\n"
   ],
   "id": "2324761703b7a703"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ee511d79327d6f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
