{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Práctica: Word2Vec / Skip-gram\n",
    "En este trabajo he implementado desde cero un modelo Word2Vec basado en Skip-Gram con el objetivo de entender su funcionamiento interno. Se ha trabajado con un corpus controlado, diseñado para contener patrones lingüísticos claros como relaciones de género gramatical, asociaciones entre agentes y acciones, y relaciones geográficas simples.\n",
    "\n",
    "Se ha utilizado softmax completo sobre todo el vocabulario, siendo consciente de su coste computacional, porque permite seguir de forma directa la función objetivo original y analizar con claridad el proceso de entrenamiento. Alternativas más eficientes como Negative Sampling no se han incluido para priorizar la comprensión conceptual del modelo.\n",
    "\n",
    "La evaluación combina un análisis cuantitativo, mediante la pérdida final y la similitud media entre palabras y sus vecinos más cercanos, con un análisis cualitativo basado en vecinos y analogías no triviales, diseñadas en función del contenido del corpus. Los resultados muestran que el modelo captura regularidades distribucionales básicas, aunque también evidencian las limitaciones derivadas del tamaño del corpus.\n",
    "Se ejecuta el proyecto en el siguiente orden:\n",
    "1. Carga y tokenización\n",
    "2. Vocabulario\n",
    "3. Pares Skip-gram (centro, contexto)\n",
    "4. Entrenamiento Skip-gram\n",
    "5. Evaluación: vecinos + analogías\n",
    "6. Comentario sobre hiperparámetros"
   ],
   "id": "930521a051f46681"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importación de librerías ",
   "id": "e9c4292ff26dfb45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T23:38:23.046959Z",
     "start_time": "2026-01-18T23:38:22.727558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "from word2vec.data import load_and_tokenize\n",
    "from word2vec.vocab import build_vocabulary\n",
    "from word2vec.pairs import generate_skipgram_pairs\n",
    "from word2vec.model import SkipGram\n",
    "from word2vec.analysis import analogy_test, average_neighbor_similarity"
   ],
   "id": "e734df18a0c4cdeb",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuración de hiperparámetros\n",
    "\n"
   ],
   "id": "fe2c1af5ad8a8797"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T23:38:23.905402Z",
     "start_time": "2026-01-18T23:38:23.900850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CORPUS_PATH = \"resources/dataset_word2vec.txt\"\n",
    "\n",
    "WINDOW_SIZE = 2\n",
    "EMBEDDING_DIM = 30\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 50\n",
    "MIN_COUNT = 1\n",
    "\n",
    "TOP_K_NEIGHBORS = 8"
   ],
   "id": "bf239a05a866da13",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Ejecución principal del programa",
   "id": "b89a47ec62027009"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T23:38:25.016956Z",
     "start_time": "2026-01-18T23:38:25.005538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) Cargar y tokenizar\n",
    "sentences, all_words = load_and_tokenize(\n",
    "    CORPUS_PATH,\n",
    "    strip_accents=False,\n",
    "    keep_numbers=False\n",
    ")\n",
    "\n",
    "# 2) Vocabulario\n",
    "vocab, inv_vocab, counter = build_vocabulary(all_words, min_count=MIN_COUNT)\n",
    "\n",
    "print(\"Tamaño del vocabulario:\", len(vocab))\n",
    "print(\"Top 10 palabras:\", counter.most_common(10))\n"
   ],
   "id": "ef8dbe17a62dfce0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 355\n",
      "Top 10 palabras: [('la', 131), ('el', 129), ('es', 37), ('está', 27), ('en', 26), ('un', 20), ('por', 15), ('una', 15), ('al', 14), ('gato', 11)]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Búsqueda de hiperparámetros\n",
    "En esta sección se entrenan varios modelos con combinaciones de hiperparámetros y se selecciona el que obtiene menor pérdida en la última época.\n"
   ],
   "id": "24e40fd86561ecb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T23:46:04.398891Z",
     "start_time": "2026-01-18T23:38:26.065584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_words = [\"perro\", \"gata\", \"coche\", \"parís\", \"francia\", \"casa\", \"agua\", \"niña\", \"profesor\", \"médico\"]\n",
    "test_indices = [vocab[w] for w in test_words if w in vocab]\n",
    "\n",
    "def score_analogies(model):\n",
    "    analogies = [\n",
    "        (\"parís\", \"francia\", \"madrid\"),\n",
    "        (\"perro\", \"ladra\", \"gato\"),\n",
    "        (\"niño\", \"niña\", \"profesor\"),\n",
    "    ]\n",
    "    score = 0.0\n",
    "    used = 0\n",
    "    for a, b, c in analogies:\n",
    "        res = analogy_test(model, vocab, inv_vocab, a, b, c, top_k=1)\n",
    "        if res is None:\n",
    "            continue\n",
    "        score += res[0][1]\n",
    "        used += 1\n",
    "    return score / used if used > 0 else 0.0\n",
    "\n",
    "\n",
    "grid = {\n",
    "    \"window_size\": [2, 3],\n",
    "    \"embedding_dim\": [20, 30, 50],\n",
    "    \"learning_rate\": [0.005, 0.01],\n",
    "    \"epochs\": [30, 50, 100],\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for ws in grid[\"window_size\"]:\n",
    "    pairs = generate_skipgram_pairs(sentences, vocab, window_size=ws)\n",
    "\n",
    "    for dim in grid[\"embedding_dim\"]:\n",
    "        for lr in grid[\"learning_rate\"]:\n",
    "            for ep in grid[\"epochs\"]:\n",
    "                model_tmp = SkipGram(len(vocab), embedding_dim=dim, learning_rate=lr)\n",
    "\n",
    "                # IMPORTANTÍSIMO: esto solo será correcto si ya corregiste el bug del return en model.train()\n",
    "                final_loss = model_tmp.train(pairs, epochs=ep, verbose_every=0)\n",
    "\n",
    "                neigh_score = average_neighbor_similarity(model_tmp, test_indices, top_k=5) if test_indices else 0.0\n",
    "                anal_score = score_analogies(model_tmp)\n",
    "\n",
    "                results.append({\n",
    "                    \"window_size\": ws,\n",
    "                    \"embedding_dim\": dim,\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"epochs\": ep,\n",
    "                    \"final_loss\": final_loss,\n",
    "                    \"avg_neighbor_sim\": neigh_score,\n",
    "                    \"avg_analogy_top1_sim\": anal_score,\n",
    "                })\n",
    "\n",
    "df_grid = pd.DataFrame(results).sort_values(\"final_loss\", ascending=True)\n",
    "df_grid.head(10)\n"
   ],
   "id": "2f4d558eb4570cff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    window_size  embedding_dim  learning_rate  epochs  final_loss  \\\n",
       "17            2             50          0.010     100    3.200688   \n",
       "11            2             30          0.010     100    3.277018   \n",
       "35            3             50          0.010     100    3.312844   \n",
       "5             2             20          0.010     100    3.337183   \n",
       "29            3             30          0.010     100    3.373771   \n",
       "23            3             20          0.010     100    3.419776   \n",
       "14            2             50          0.005     100    3.951078   \n",
       "16            2             50          0.010      50    3.995230   \n",
       "8             2             30          0.005     100    4.020109   \n",
       "10            2             30          0.010      50    4.061947   \n",
       "\n",
       "    avg_neighbor_sim  avg_analogy_top1_sim  \n",
       "17          0.779667              0.890657  \n",
       "11          0.796621              0.920835  \n",
       "35          0.793965              0.853818  \n",
       "5           0.813248              0.921360  \n",
       "29          0.811830              0.889943  \n",
       "23          0.826954              0.888906  \n",
       "14          0.880219              0.963678  \n",
       "16          0.882428              0.963829  \n",
       "8           0.898343              0.971051  \n",
       "10          0.901169              0.972566  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window_size</th>\n",
       "      <th>embedding_dim</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epochs</th>\n",
       "      <th>final_loss</th>\n",
       "      <th>avg_neighbor_sim</th>\n",
       "      <th>avg_analogy_top1_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.010</td>\n",
       "      <td>100</td>\n",
       "      <td>3.200688</td>\n",
       "      <td>0.779667</td>\n",
       "      <td>0.890657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>0.010</td>\n",
       "      <td>100</td>\n",
       "      <td>3.277018</td>\n",
       "      <td>0.796621</td>\n",
       "      <td>0.920835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.010</td>\n",
       "      <td>100</td>\n",
       "      <td>3.312844</td>\n",
       "      <td>0.793965</td>\n",
       "      <td>0.853818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0.010</td>\n",
       "      <td>100</td>\n",
       "      <td>3.337183</td>\n",
       "      <td>0.813248</td>\n",
       "      <td>0.921360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>0.010</td>\n",
       "      <td>100</td>\n",
       "      <td>3.373771</td>\n",
       "      <td>0.811830</td>\n",
       "      <td>0.889943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0.010</td>\n",
       "      <td>100</td>\n",
       "      <td>3.419776</td>\n",
       "      <td>0.826954</td>\n",
       "      <td>0.888906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.005</td>\n",
       "      <td>100</td>\n",
       "      <td>3.951078</td>\n",
       "      <td>0.880219</td>\n",
       "      <td>0.963678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.010</td>\n",
       "      <td>50</td>\n",
       "      <td>3.995230</td>\n",
       "      <td>0.882428</td>\n",
       "      <td>0.963829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>0.005</td>\n",
       "      <td>100</td>\n",
       "      <td>4.020109</td>\n",
       "      <td>0.898343</td>\n",
       "      <td>0.971051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>0.010</td>\n",
       "      <td>50</td>\n",
       "      <td>4.061947</td>\n",
       "      <td>0.901169</td>\n",
       "      <td>0.972566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Mejor resultado de hiperparámetros",
   "id": "50f891350ebcb7fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T23:46:24.144817Z",
     "start_time": "2026-01-18T23:46:24.139304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best = df_grid.iloc[0]\n",
    "print(\"Mejor combinación encontrada (menor pérdida final):\")\n",
    "print(best)\n",
    "\n",
    "best_ws  = int(best[\"window_size\"])\n",
    "best_dim = int(best[\"embedding_dim\"])\n",
    "best_lr  = float(best[\"learning_rate\"])\n",
    "best_ep  = int(best[\"epochs\"])"
   ],
   "id": "cb238989387e0af7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor combinación encontrada (menor pérdida final):\n",
      "window_size               2.000000\n",
      "embedding_dim            50.000000\n",
      "learning_rate             0.010000\n",
      "epochs                  100.000000\n",
      "final_loss                3.200688\n",
      "avg_neighbor_sim          0.779667\n",
      "avg_analogy_top1_sim      0.890657\n",
      "Name: 17, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Reentrenar el modelo con los mejores hiperparámetros\n",
    "Se reentrena un modelo final con la mejor configuración para obtener las mejores analogías y vecinos."
   ],
   "id": "be6529a166a48075"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T23:46:50.836715Z",
     "start_time": "2026-01-18T23:46:28.049921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_pairs = generate_skipgram_pairs(sentences, vocab, window_size=best_ws)\n",
    "\n",
    "best_model = SkipGram(len(vocab), embedding_dim=best_dim, learning_rate=best_lr)\n",
    "best_final_loss = best_model.train(best_pairs, epochs=best_ep, verbose_every=10)\n",
    "\n",
    "print(\"\\nPérdida final del modelo seleccionado:\", best_final_loss)\n"
   ],
   "id": "3ad8cf34e5dd37c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: Loss: 5.5937\n",
      "Epoch 20/100: Loss: 4.7716\n",
      "Epoch 30/100: Loss: 4.4778\n",
      "Epoch 40/100: Loss: 4.2305\n",
      "Epoch 50/100: Loss: 3.9947\n",
      "Epoch 60/100: Loss: 3.7803\n",
      "Epoch 70/100: Loss: 3.6031\n",
      "Epoch 80/100: Loss: 3.4520\n",
      "Epoch 90/100: Loss: 3.3172\n",
      "Epoch 100/100: Loss: 3.2016\n",
      "\n",
      "Pérdida final del modelo seleccionado: 3.201640312171827\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Vecinos más cercanos",
   "id": "fe2ca90589701c8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T23:48:01.482285Z",
     "start_time": "2026-01-18T23:48:01.467372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def nearest_neighbors(words, top_k=8):\n",
    "    rows = []\n",
    "    for w in words:\n",
    "        if w not in vocab:\n",
    "            rows.append({\"Palabra\": w, f\"Vecinos (top-{top_k})\": \"(no está en el vocabulario)\"})\n",
    "            continue\n",
    "        idx = vocab[w]\n",
    "        nn = best_model.nearest_neighbors(idx, top_k=top_k)\n",
    "        neigh_words = [inv_vocab[j] for j, _ in nn]\n",
    "        rows.append({\"Palabra\": w, f\"Vecinos (top-{top_k})\": \", \".join(neigh_words)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "target_words = [\"perro\", \"gata\", \"coche\", \"parís\", \"francia\"]\n",
    "nearest_neighbors(target_words, top_k=TOP_K_NEIGHBORS)\n"
   ],
   "id": "8b2eb77a7563baff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Palabra                                    Vecinos (top-8)\n",
       "0    perro  gato, ratón, pequeño, perra, verdura, gata, la...\n",
       "1     gata  perra, cuerda, cacao, alumna, hermana, bebo, v...\n",
       "2    coche  lento, rápido, cuadro, verano, pasa, espera, c...\n",
       "3    parís  madrid, roma, españa, francia, italia, ciudad,...\n",
       "4  francia  italia, españa, palacio, madrid, parís, roma, ..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palabra</th>\n",
       "      <th>Vecinos (top-8)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>perro</td>\n",
       "      <td>gato, ratón, pequeño, perra, verdura, gata, la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gata</td>\n",
       "      <td>perra, cuerda, cacao, alumna, hermana, bebo, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coche</td>\n",
       "      <td>lento, rápido, cuadro, verano, pasa, espera, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>parís</td>\n",
       "      <td>madrid, roma, españa, francia, italia, ciudad,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>francia</td>\n",
       "      <td>italia, españa, palacio, madrid, parís, roma, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analogías",
   "id": "863cd255d7994350"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T23:48:12.691699Z",
     "start_time": "2026-01-18T23:48:12.682942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "analogies = [\n",
    "    (\"parís\", \"francia\", \"madrid\"),\n",
    "    (\"perro\", \"ladra\", \"gato\"),\n",
    "    (\"niño\", \"niña\", \"profesor\"),\n",
    "]\n",
    "\n",
    "for a, b, c in analogies:\n",
    "    res = analogy_test(best_model, vocab, inv_vocab, a, b, c, top_k=3)\n",
    "    print(f\"\\n{a} : {b} :: {c} : ?\")\n",
    "    if res is None:\n",
    "        print(\"  (alguna palabra no está en el vocabulario)\")\n",
    "    else:\n",
    "        for w, sim in res:\n",
    "            print(f\"  - {w}: {sim:.4f}\")\n"
   ],
   "id": "8a5e361dd398b9f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "parís : francia :: madrid : ?\n",
      "  - españa: 0.9985\n",
      "  - italia: 0.9985\n",
      "  - palacio: 0.8780\n",
      "\n",
      "perro : ladra :: gato : ?\n",
      "  - maúlla: 0.7821\n",
      "  - pequeño: 0.7149\n",
      "  - persigue: 0.7049\n",
      "\n",
      "niño : niña :: profesor : ?\n",
      "  - profesora: 0.9054\n",
      "  - bicicleta: 0.8365\n",
      "  - moto: 0.8229\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conclusiones\n",
    "\n",
    "### Análisis general\n",
    "Los resultados que se han obtenido muestran que el modelo Skip-gram es capaz de aprender relaciones básicas entre palabras a partir del corpus proporcionado, que es bastante sencillo y no muy grande. En general, los vecinos más cercanos de una palabra suelen estar relacionados con su significado o con su uso habitual, como ocurre en ejemplos del tipo perro–gato o parís–francia–madrid-españa. Esto indica que los embeddings aprendidos capturan información relevante del contexto en el que aparecen las palabras.\n",
    "\n",
    "### Análisis de analogías\n",
    "Las analogías permiten observar de forma clara este comportamiento. Más concretamente, el modelo es capaz de resolver correctamente analogías sencillas como parís : francia :: madrid : españa, así como algunas transformaciones relacionadas con el género gramatical, por ejemplo niño : niña :: profesor : profesora. Aunque no todas las analogías funcionan perfectamente, los resultados obtenidos son razonables teniendo en cuenta que el corpus no es muy grande.\n",
    "\n",
    "### Análisis de hiperparámetros\n",
    "El análisis de hiperparámetros muestra que la elección de los mismos juegan un papel importante en el desarrollo del programa. En este caso, la mejor configuración corresponde a un tamaño de ventana de contexto intermedio, una dimensión de embedding relativamente grande y un número elevado de épocas de entrenamiento. Un tamaño de ventana de 2 permite capturar suficiente información del contexto sin introducir demasiado ruido, mientras que una dimensión de embedding mayor proporciona al modelo más capacidad para representar relaciones entre palabras. Por su parte, entrenar durante más épocas permite que el modelo ajuste mejor los vectores y reduzca la pérdida final, siempre que el corpus no sea excesivamente grande.\n",
    "\n",
    "### Conclusión general\n",
    "Por otro lado, el trabajo también nos muestra que existen algunas limitaciones. El corpus que se utiliza es pequeño, lo que restringe la complejidad de las relaciones que el modelo puede aprender, y el uso de softmax completo hace que el entrenamiento sea más lento y poco escalable. Estas limitaciones son esperables en el contexto de la práctica y no invalidan los resultados obtenidos.\n"
   ],
   "id": "2324761703b7a703"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "384399eefba16e80"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
