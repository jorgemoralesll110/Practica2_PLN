{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PRÁCTICA 2: word2vec / skip-gram\n",
    "### Miembros: Raquel Almeida Quesada y Jorge Morales Llerandi\n"
   ],
   "id": "4fd7e52dcbe61727"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Cargar y tokenizar el corpus  ",
   "id": "d8e4bfe6e95f98d6"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "with open(\"resources/dataset_word2vec.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().lower()\n",
    "    \n",
    "tokens = re.findall(r'\\b[a-záéíóúüñ]+\\b', text)\n",
    "print(\"Número total de tokens:\", len(tokens))\n",
    "\n",
    "tokens\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Crear vocabulario y los pares (centro, contexto)",
   "id": "b60725c655e2244b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from itertools import chain\n",
    "\n",
    "window_size = 2\n",
    "vocab = list(set(tokens))\n",
    "word_to_ix = {w: i  for i, w in enumerate(vocab)}\n",
    "ix_to_word = {i: w for w, i in word_to_ix.items()}\n",
    "\n",
    "pairs = []\n",
    "for i, center in enumerate(tokens):\n",
    "    for j in range(max(0, i - window_size), min(len(tokens), i + window_size + 1)):\n",
    "        if i != j:\n",
    "            pairs.append((center, tokens[j]))\n",
    "            \n",
    "print(\"Ejemplo de par:\", pairs[:10])\n",
    "            "
   ],
   "id": "2d08ab1854f27688",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Crear el dataset para Pytorch",
   "id": "69099463e683a3a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, pairs, word_to_ix):\n",
    "        self.pairs = pairs\n",
    "        self.word_to_ix = word_to_ix\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        center, context = self.pairs[idx]\n",
    "        return torch.tensor(self.word_to_ix[center]), torch.tensor(self.word_to_ix[context])\n",
    "    \n",
    "dataset = Word2VecDataset(pairs, word_to_ix)\n",
    "dataLoader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ],
   "id": "c9b738233ba44205",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Definir el modelo Skip-Gram",
   "id": "73f0d858408d8aa6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.output = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    def forward(self, center_words):\n",
    "        embeds = self.embedding(center_words)\n",
    "        out = self.output(embeds)\n",
    "        return out\n",
    "    \n",
    "embedding_dim = 50\n",
    "model = SkipGramModel(len(vocab), embedding_dim)"
   ],
   "id": "394e22fe712fa10c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Entrenamiento",
   "id": "550065eba64b435e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for center, context in dataLoader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(center)\n",
    "        loss = criterion(output, context)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calcular accuracy (predicción correcta del contexto)\n",
    "        preds = torch.argmax(output, dim=1)\n",
    "        correct += (preds == context).sum().item()\n",
    "        total += context.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataLoader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Época {epoch+1} | Pérdida media: {avg_loss:.4f} | Precisión: {accuracy:.2f}%\")\n"
   ],
   "id": "9b3b67f25f134ab5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Explorar los embeddings (vecinos más cercanos)",
   "id": "42f5428bdd0b5a35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_embedding(word):\n",
    "    idx = word_to_ix[word]\n",
    "    return model.embeddings.weight[idx]\n",
    "\n",
    "def nearest(word, top_n=5):\n",
    "    word_emb = get_embedding(word)\n",
    "    sims = F.cosine_similarity(word_emb.unsqueeze(0), model.embeddings.weight)\n",
    "    best = torch.topk(sims, top_n+1).indices.tolist()[1:]\n",
    "    return [ix_to_word[i] for i in best]\n",
    "\n",
    "print(\"Vecinos de 'parís':\", nearest(\"parís\"))\n"
   ],
   "id": "d4c7553ad08cd9f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Pruebas de analogías",
   "id": "5cf8ebeaa725bf2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def analogy(w1, w2, w3, top_n=1):\n",
    "    emb = get_embedding(w2) - get_embedding(w1) + get_embedding(w3)\n",
    "    sims = F.cosine_similarity(emb.unsqueeze(0), model.embeddings.weight)\n",
    "    best = torch.topk(sims, top_n+3).indices.tolist()\n",
    "    result = [ix_to_word[i] for i in best if ix_to_word[i] not in [w1, w2, w3]][:top_n]\n",
    "    return result\n",
    "\n",
    "print(\"parís : francia :: madrid : ?\", analogy(\"parís\", \"francia\", \"madrid\"))\n"
   ],
   "id": "23c3c191a1d5242",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bba64def615e7dc5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
